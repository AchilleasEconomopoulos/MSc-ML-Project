{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.data_loader import load_csv,custom_tokenizer, transform_input\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, roc_curve, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "import ipywidgets as widgets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset & Util Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex_scores = []\n",
    "\n",
    "#Load the dreaddit dataset, spellchecked and with the avg lexicon score added as a column\n",
    "texts, labels, lex_scores = load_csv('data/dreaddit-train-spellchecked-lex.csv')\n",
    "\n",
    "# Uncomment for training without lexicon scores\n",
    "# texts, labels = load_csv('data/dreaddit-train-spellchecked.csv', lexicon=False)\n",
    "\n",
    "\n",
    "# The tool used to Tokenize and calculate the samples' Tfidf\n",
    "vectorizer_params = {\n",
    "    # \"stop_words\": \"english\",      # SKLearn's default stopword removal\n",
    "    \"min_df\": 3,                    # min no. of documents allowed\n",
    "    \"max_df\": 0.2,                  # max % of documents allowed \n",
    "    \"max_features\": 3000,           # retains X best performing tokens\n",
    "    \"ngram_range\":(1,3),            # 1 token can be 1-3 words\n",
    "    \"tokenizer\": custom_tokenizer,  # custom regex & stemming\n",
    "    \"token_pattern\": None           # To suppress warnings\n",
    "}\n",
    "\n",
    "vectorizer = TfidfVectorizer(**vectorizer_params) # SKLearn's vectorizer\n",
    "scaler = MinMaxScaler() # SKLearn's MinMax scaler initialization (to normalize the lexicon avg scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''The K-Fold validation pipeline.'''\n",
    "\n",
    "kf = KFold()\n",
    "\n",
    "# Multinomial NB, Logistic Regression as fast, interpretable models\n",
    "# SVC and XGBoost for non-linear feature relationships\n",
    "models = { \n",
    "          \"Multinomial NB\": {\"clf\": MultinomialNB(), \"scores\": [], \"feature_importance\" : []},\n",
    "          \"Logistic Regression\" : {\"clf\": LogisticRegression(C=0.3), \"scores\": [], \"feature_importance\" : []},\n",
    "          # \"SVC\" : {\"clf\": svm.SVC(kernel='rbf', probability=True), \"scores\": [], \"feature_importance\" : []},\n",
    "          \"XGBoost\" : {\"clf\": XGBClassifier(n_estimators=200,\n",
    "                              learning_rate=0.05,\n",
    "                              max_depth=4,\n",
    "                              gamma=2,\n",
    "                              subsample=0.8,\n",
    "                              colsample_bytree=0.8), \"scores\": [], \"feature_importance\" : []}\n",
    "          }\n",
    "\n",
    "\n",
    "# Start to iterate the KFold splits\n",
    "for train_ids, val_ids in kf.split(texts):\n",
    "\n",
    "    # Retrieves the corresponding samples based on the k-fold split\n",
    "    X_train_texts = [texts[i] for i in train_ids]\n",
    "    X_val_texts = [texts[i] for i in val_ids]\n",
    "    y_train = [labels[i] for i in train_ids]\n",
    "    y_val = [labels[i] for i in val_ids]\n",
    "    \n",
    "    \n",
    "    X_train = vectorizer.fit_transform(X_train_texts)   # Fits the vectorizer on the training data\n",
    "    X_val = vectorizer.transform(X_val_texts)           # Uses the fitted vectorizer on the validation data (no retraining)\n",
    "\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Condition added in case I want to run the pipeline for the initial / spellchecked dataset without the lexicon scores\n",
    "    if any(lex_scores):\n",
    "\n",
    "      # Retrieve the scores of the train and validation data separately\n",
    "      X_train_avg_lex_scores = np.array([lex_scores[i] for i in train_ids]).reshape(-1,1)\n",
    "      X_val_avg_lex_scores = np.array([lex_scores[i] for i in val_ids]).reshape(-1,1)\n",
    "\n",
    "      \n",
    "      # Normalization to bring the lexicon scores on the same scale as the Tfidf features\n",
    "      X_train_lex_scores = scaler.fit_transform(X_train_avg_lex_scores.reshape(-1, 1))  # Fits the scaler on the training lex_scores and normalizes them [0,1]\n",
    "      X_val_lex_scores = scaler.transform(X_val_avg_lex_scores.reshape(-1,1))           # Normalizes the validation lex_scores according to the training data normalization\n",
    "\n",
    "\n",
    "      # Adds the extra lexicon feature at the end of the base Tfidf features (horizontal stacking)\n",
    "      X_train = hstack([X_train, X_train_lex_scores])\n",
    "      X_val = hstack([X_val, X_val_lex_scores])\n",
    "\n",
    "      # Training and validation transformed samples are given as a Dataframe to keep track of the actual token each feature represents\n",
    "      # This helps understand what the \"top k performing features\" output of each model actually means for the vocabulary \n",
    "      X_train = pd.DataFrame(X_train.toarray(), columns=np.append(vectorizer.get_feature_names_out(),\"lexicon_score\"))\n",
    "      X_val = pd.DataFrame(X_val.toarray(), columns=np.append(vectorizer.get_feature_names_out(),\"lexicon_score\"))\n",
    "\n",
    "\n",
    "    # A loop to train and evaluate every model\n",
    "    for model_name, model in models.items():\n",
    "        clf = model['clf'].fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_val)\n",
    "        y_score = clf.predict_proba(X_val)\n",
    "        model['scores'].append(f1_score(y_val, y_pred, average='macro'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # FEATURE IMPORTANCE FOR VISUALIZATION LATER\n",
    "\n",
    "    # XGBoost natively provides feature importance\n",
    "    xgboost_feature_scores = models['XGBoost']['clf'].get_booster().get_score(importance_type='total_gain')                         #Total gain = sum of info gain across samples\n",
    "    xgb_df = pd.DataFrame(list(xgboost_feature_scores.items()), columns=[\"Token\", \"Score\"]).sort_values(\"Score\", ascending=False)\n",
    "    models['XGBoost']['feature_importance'].append(xgb_df)\n",
    "\n",
    "\n",
    "    # Feature importance in LR is given by the calculated weights for each feature\n",
    "    importance = models['Logistic Regression']['clf'].coef_[0]  # Coefficients per feature\n",
    "    feature_names = np.append(feature_names,'lexicon_score')\n",
    "    top_features = pd.DataFrame({\"Word\": feature_names, \"Weight\": importance}).sort_values(\"Weight\", ascending=False)\n",
    "    models['Logistic Regression']['feature_importance'].append(top_features)\n",
    "    \n",
    "    # Feature importance \n",
    "    log_prob = models['Multinomial NB']['clf'].feature_log_prob_\n",
    "    nb_features = pd.DataFrame({\"Token\": feature_names, \"Class 0\":log_prob[0], \"Class 1\": log_prob[1]}).sort_values(by=\"Class 1\", ascending=False)\n",
    "    models['Multinomial NB']['feature_importance'].append(nb_features)\n",
    "\n",
    "\n",
    "    \n",
    "    ## ROC Curves for every validation fold ---> An attempt at threshold tuning\n",
    "  \n",
    "    # plt.figure(figsize=(10, 7))\n",
    "    # for model_name, model in models.items():\n",
    "    #     fpr, tpr, _ = roc_curve(y_val,model['y_scores'][:,1])\n",
    "    #     roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    #     plt.plot(fpr, tpr, label=f'{model_name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    # plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line\n",
    "    # plt.xlim([0.0, 1.0])\n",
    "    # plt.ylim([0.0, 1.05])\n",
    "    # plt.xlabel('False Positive Rate')\n",
    "    # plt.ylabel('True Positive Rate')\n",
    "    # plt.title('ROC Curves for Models')\n",
    "    # plt.legend(loc='lower right')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Score Means and Variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculates the f1-score mean and variance of each model.\n",
    "It then creates a dataframe for visualization.\n",
    "'''\n",
    "\n",
    "df_data = {'Classifier': [], 'Mean':[], \"Variance\":[], \"Scores\":[]}\n",
    "for model_name, model in models.items():\n",
    "    mean = np.mean(model['scores'])\n",
    "    var = np.var(model['scores'])\n",
    "\n",
    "    df_data['Classifier'].append(model_name)\n",
    "    df_data['Mean'].append(mean)\n",
    "    df_data['Variance'].append(var)\n",
    "    df_data['Scores'].append(model['scores'])\n",
    "    \n",
    "scores_df = pd.DataFrame(df_data)\n",
    "print(scores_df)\n",
    "\n",
    "# Used to keep (some) track of the models' performance when tuning the vectorizer's parameters.\n",
    "with open('scores.txt','a') as f:\n",
    "    f.write(f\"\\n\\n{vectorizer_params}\")\n",
    "    f.write(f\"\\n{scores_df.to_string()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Visualizes the above dataframe with a Box plot.'''\n",
    "\n",
    "scores = []\n",
    "plot_labels = []\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "for model_name, model in models.items():\n",
    "    scores.append(model['scores'])\n",
    "    plot_labels.append(model_name)\n",
    "    \n",
    "plt.boxplot(scores,patch_artist=True,tick_labels=plot_labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance per fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' VISUALIZATION OF FEATURE IMPORTANCE FOR EACH FOLD, FOR EACH MODEL'''\n",
    "\n",
    "# Select the amount of top features to return\n",
    "k = 5\n",
    "\n",
    "output1 = widgets.Output()\n",
    "output2 = widgets.Output()\n",
    "output3 = widgets.Output()\n",
    "# Number of folds\n",
    "for i in range(5):\n",
    "\n",
    "    xgb_df = models['XGBoost']['feature_importance'][i]\n",
    "    lr_df = models['Logistic Regression']['feature_importance'][i]\n",
    "    nb_df = models['Multinomial NB']['feature_importance'][i]\n",
    "\n",
    "    with output1: display(xgb_df[0:k])\n",
    "    with output2: display(lr_df[0:k])\n",
    "    with output3: display(nb_df[0:k])\n",
    "\n",
    "box1 = widgets.VBox([widgets.Label(\"XGBoost\"), output1], layout=widgets.Layout(align_items='center'))\n",
    "box2 = widgets.VBox([widgets.Label(\"Logistic Regression\"), output2], layout=widgets.Layout(align_items='center'))\n",
    "box3 = widgets.VBox([widgets.Label(\"Multinomial NB\"), output3], layout=widgets.Layout(align_items='center'))\n",
    "\n",
    "three_columns = widgets.HBox([box1,box2,box3])\n",
    "display(three_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Follows a very similar worfklow to the K-fold validation one.\n",
    "Here however we use the whole training set for training and test set for evaluation.\n",
    "'''\n",
    "\n",
    "texts, y_train, train_lex_scores = load_csv('data/dreaddit-train-spellchecked-lex.csv')\n",
    "test_texts, y_test, test_lex_scores = load_csv(\"./data/dreaddit-test-spellchecked-lex.csv\")\n",
    "\n",
    "# Uncomment for training without lexicon scores\n",
    "# texts, labels = load_csv('data/dreaddit-train-spellchecked.csv', lexicon=False)\n",
    "# test_texts, y_test = load_csv(\"./data/dreaddit-test-spellchecked.csv\", lexicon=False)\n",
    "\n",
    "models = { \n",
    "          \"Multinomial NB\": {\"clf\": MultinomialNB()},\n",
    "          \"Logistic Regression\" : {\"clf\": LogisticRegression(C=0.3),},\n",
    "          \"SVC\" : {\"clf\": svm.SVC(kernel='rbf', probability=True)},     # SVM enabled here\n",
    "          \"XGBoost\" : {\"clf\": XGBClassifier(n_estimators=200,\n",
    "                              learning_rate=0.05,\n",
    "                              max_depth=4,\n",
    "                              gamma=2,\n",
    "                              subsample=0.8,\n",
    "                              colsample_bytree=0.8)}\n",
    "          }\n",
    "\n",
    "X_train = vectorizer.fit_transform(texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "\n",
    "if any(lex_scores):\n",
    "\n",
    "    X_train_avg_lex_scores = train_lex_scores\n",
    "    X_test_avg_lex_scores = test_lex_scores\n",
    "    \n",
    "    X_train_lex_scores = scaler.fit_transform(X_train_avg_lex_scores.reshape(-1, 1))\n",
    "    X_test_lex_scores = scaler.transform(X_test_avg_lex_scores.reshape(-1,1))\n",
    "\n",
    "    X_train = hstack([X_train, X_train_lex_scores])\n",
    "    X_test = hstack([X_test, X_test_lex_scores])\n",
    "\n",
    "\n",
    "    X_train = pd.DataFrame(X_train.toarray(), columns=np.append(vectorizer.get_feature_names_out(),\"lexicon_score\"))\n",
    "    X_test = pd.DataFrame(X_test.toarray(), columns=np.append(vectorizer.get_feature_names_out(),\"lexicon_score\"))\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    clf = model['clf'].fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_score = clf.predict_proba(X_test)\n",
    "    model['y_pred'] = y_pred\n",
    "    model['y_score'] = y_score\n",
    "    model['f1_score'] = f1_score(y_test, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set f1 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_data = {'Classifier': [], \"f1_score\":[]}\n",
    "for model_name, model in models.items():\n",
    "    df_data['Classifier'].append(model_name)\n",
    "    df_data['f1_score'].append(model['f1_score'])\n",
    "    \n",
    "scores_df = pd.DataFrame(df_data)\n",
    "print(scores_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrices = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    confusion_matrices[model_name] = confusion_matrix(y_test,model['y_pred'])\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(15, 5))\n",
    "axes= axes.flatten()\n",
    "for i, (name,cm) in enumerate(confusion_matrices.items()):\n",
    "    subplot = ConfusionMatrixDisplay(cm)\n",
    "    axes[i].set_title(name)\n",
    "    subplot.plot(ax=axes[i], cmap='Blues', colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = []\n",
    "tpr = []\n",
    "roc_auc = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    m_fpr, m_tpr, _ = roc_curve(y_test,model['y_score'][:,1]) # Convert labels to one-hot encoding (required for ROC-AUC computation)\n",
    "    m_roc_auc = auc(m_fpr,m_tpr)\n",
    "    fpr.append(m_fpr)\n",
    "    tpr.append(m_tpr)\n",
    "    roc_auc.append(m_roc_auc)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr[0],tpr[0], label=f'MultinomialNB (AUC = {roc_auc[0]:.2f})', color = 'orange')\n",
    "plt.plot(fpr[1], tpr[1], label=f'Logistic Regression (AUC = {roc_auc[1]:.2f})', color='blue')\n",
    "plt.plot(fpr[2],tpr[2], label=f'SVC(RBF) (AUC = {roc_auc[2]:.2f})', color = 'red')\n",
    "plt.plot(fpr[3],tpr[3], label=f'XGBoost (AUC = {roc_auc[3]:.2f})', color = 'purple')\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Random guess line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve for Binary Classification\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_texts = [\"I am very stressed and I have anxiety.\",\"Today I went for a walk in the park. it was Amazing!. I feel very happy and I can't wait to go tomorrow as well.\"]\n",
    "\n",
    "for text in inference_texts:\n",
    "    print(f\"Inference text: {text}\")\n",
    "\n",
    "    input = transform_input(text,vectorizer,scaler)\n",
    "\n",
    "    # print(models['Multinomial NB']['clf'].predict(input))\n",
    "    print(models['Multinomial NB']['clf'].predict_proba(input))\n",
    "\n",
    "    # print(models['Logistic Regression']['clf'].predict(input))\n",
    "    print(models['Logistic Regression']['clf'].predict_proba(input))\n",
    "\n",
    "    # print(models['SVC']['clf'].predict(input))\n",
    "    print(models['SVC']['clf'].predict_proba(input))\n",
    "\n",
    "    # print(models['XGBoost']['clf'].predict(input))\n",
    "    print(models['XGBoost']['clf'].predict_proba(input))\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
